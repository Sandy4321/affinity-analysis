{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apriori Algorithm\n",
    "\n",
    "- a classic algorithm used in data mining for learning association rules\n",
    "- it can be used to find items that are purchased together more frequently than others\n",
    "\n",
    "## Useful tips\n",
    "\n",
    "### Mapping through rows and columns in an array\n",
    "\n",
    "Create an `np.vectorize` to perform mapping. Let's say we are given an array of items, and for each rows and columns, we want to just take the initials:\n",
    "\n",
    "```python\n",
    "items = np.array([['Mango', 'Onion', 'Nintendo', 'Key-chain', 'Eggs', 'Yo-yo'],\n",
    "                  ['Doll', 'Onion', 'Nintendo', 'Key-chain', 'Eggs', 'Yo-yo'],\n",
    "                  ['Mango', 'Apple', 'Key-chain', 'Eggs'],\n",
    "                  ['Mango', 'Umbrella', 'Corn', 'Key-chain', 'Yo-yo'],\n",
    "                  ['Corn', 'Onion', 'Onion', 'Key-chain', 'Ice-cream', 'Eggs']])\n",
    "\n",
    "# Create a new np function that takes the first character from each\n",
    "# items in the array (for simplification)\n",
    "take_first = lambda x: x[0]\n",
    "f = np.vectorize(take_first)\n",
    "\n",
    "# Apply the function to the items\n",
    "# Note that we also use frozenset to remove duplicates from each transaction\n",
    "data = [frozenset(f(i)) for i in items]\n",
    "```\n",
    "\n",
    "Output:\n",
    "\n",
    "```python\n",
    "[frozenset({'E', 'K', 'M', 'N', 'O', 'Y'}),\n",
    " frozenset({'D', 'E', 'K', 'N', 'O', 'Y'}),\n",
    " frozenset({'A', 'E', 'K', 'M'}),\n",
    " frozenset({'C', 'K', 'M', 'U', 'Y'}),\n",
    " frozenset({'C', 'E', 'I', 'K', 'O'})]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from functools import reduce\n",
    "from itertools import combinations\n",
    "\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Every row is a transaction, and every column represent the item bought\n",
    "# Note that in a single transaction, there can be similar items bought\n",
    "items = np.array([['Mango', 'Onion', 'Nintendo', 'Key-chain', 'Eggs', 'Yo-yo'],\n",
    "                  ['Doll', 'Onion', 'Nintendo', 'Key-chain', 'Eggs', 'Yo-yo'],\n",
    "                  ['Mango', 'Apple', 'Key-chain', 'Eggs'],\n",
    "                  ['Mango', 'Umbrella', 'Corn', 'Key-chain', 'Yo-yo'],\n",
    "                  ['Corn', 'Onion', 'Onion', 'Key-chain', 'Ice-cream', 'Eggs']])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each index `i` will represent a class:\n",
      "\n",
      "0 => Apple\n",
      "1 => Corn\n",
      "2 => Doll\n",
      "3 => Eggs\n",
      "4 => Ice-cream\n",
      "5 => Key-chain\n",
      "6 => Mango\n",
      "7 => Nintendo\n",
      "8 => Onion\n",
      "9 => Umbrella\n",
      "10 => Yo-yo\n"
     ]
    }
   ],
   "source": [
    "# Create a new label encoder to learn the mappings\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Fit the mappings to learn them\n",
    "le.fit(np.hstack(items))\n",
    "# le.fit(list(set(np.hstack(items)))) # Will this be more performant?\n",
    "\n",
    "print('Each index `i` will represent a class:\\n')\n",
    "for i, v in enumerate(le.classes_):\n",
    "    print('{} => {}'.format(i, v))\n",
    "\n",
    "# le.transform(['A', 'M'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[frozenset({3, 5, 6, 7, 8, 10}),\n",
       " frozenset({2, 3, 5, 7, 8, 10}),\n",
       " frozenset({0, 3, 5, 6}),\n",
       " frozenset({1, 5, 6, 9, 10}),\n",
       " frozenset({1, 3, 4, 5, 8})]"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now that we have learned the mappings, let's apply it to our dataset\n",
    "# We will also remove duplicate items from the array using frozenset\n",
    "# There is an added advantage of being able to check intersection too between sets using frozenset\n",
    "encoded_items = [frozenset(le.transform(i)) for i in items]\n",
    "encoded_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check: 2\n",
      "check: 3\n",
      "no more items to recommend\n",
      "If you buy Eggs, Key-chain, you might also like Onion\n",
      "If you buy Eggs, Key-chain, you might also like Yo-yo\n",
      "If you buy Eggs, Key-chain, you might also like Mango\n",
      "If you buy Key-chain, Mango, you might also like Yo-yo\n",
      "If you buy Onion, Key-chain, you might also like Yo-yo\n"
     ]
    }
   ],
   "source": [
    "class Apriori:\n",
    "    def __init__(self, data = None, min_support = 2):\n",
    "        self.data = data\n",
    "        self.min_support = min_support\n",
    "        pass\n",
    "\n",
    "    def score(self):\n",
    "        out = {}\n",
    "        recs = {}\n",
    "        \n",
    "        singles = reduce(lambda x, y: x + y, self.data)\n",
    "        singles_counter = self.count(singles, self.min_support)\n",
    "        \n",
    "        out[1] = singles_counter\n",
    "        \n",
    "        for i in range(2, 10):\n",
    "            pairs = self.combination(out[i - 1], i)\n",
    "            pairs_support = self.check_exist(pairs)\n",
    "            pairs_counter = self.count(pairs_support, self.min_support)\n",
    "            if len(pairs_counter) == 0:\n",
    "                print('no more items to recommend')\n",
    "                break\n",
    "            print('check:', i)\n",
    "            \n",
    "            out[i] = pairs_counter\n",
    "            recs[i] = self.recommend(pairs_counter)\n",
    "        return recs\n",
    "    \n",
    "    def check_exist(self, data):\n",
    "        supports = []\n",
    "        for i in data:\n",
    "            i_fs = frozenset(i)\n",
    "            for j in self.data:\n",
    "                j_fs = frozenset(j)\n",
    "                if i_fs.issubset(j_fs):\n",
    "                    supports.append(i_fs)\n",
    "                    continue\n",
    "        return supports\n",
    "\n",
    "    def flatten(self, data):\n",
    "        if isinstance(data, list):\n",
    "            if isinstance(data[0], list):\n",
    "                return reduce(lambda x, y: x + y, data)\n",
    "        return data\n",
    "\n",
    "    def recommend(self, data):\n",
    "        recs = []\n",
    "        recs_tmp = []\n",
    "        cnt = Counter()\n",
    "        \n",
    "        for i in data:\n",
    "            i_fs = frozenset(i)\n",
    "            for j in self.data:\n",
    "                data_fs = frozenset(j)\n",
    "                if i_fs.issubset(data_fs):\n",
    "                    to_recommend = data_fs.difference(i_fs)\n",
    "                    # The difference can be more than 2, but if we want\n",
    "                    # to recommend only 1 item, we have to loop through it\n",
    "                    for _, k in enumerate(list(to_recommend)):\n",
    "                        union = i_fs.union([k])\n",
    "                        if cnt[union] == 0:\n",
    "                            recs_tmp.append((list(i), union))\n",
    "                        cnt[union] += 1\n",
    "\n",
    "        for i, to_recommend in recs_tmp:\n",
    "            if cnt[to_recommend] > self.min_support:\n",
    "                recs.append([i, list(to_recommend.difference(i))])\n",
    "        return recs\n",
    "        \n",
    "    def count(self, data, min_support):\n",
    "        cnt = Counter(data)\n",
    "        support = {k: v for k, v in list(cnt.items())\n",
    "                   if v > min_support}\n",
    "\n",
    "        keys = list(support.keys())\n",
    "        for i, v in enumerate(keys):\n",
    "            if isinstance(v, frozenset):\n",
    "                keys[i] = list(v)\n",
    "        return keys\n",
    "    \n",
    "    def combination(self, data, r):\n",
    "        '''Returns the unique combination of data'''\n",
    "        # Flatten data if necessary to 1d\n",
    "        if isinstance(data[0], list):\n",
    "            data = list(set(reduce(lambda x, y: x + y, data)))\n",
    "        combs = combinations(data, r = r)\n",
    "        \n",
    "        # We want an array, not tuples\n",
    "        combs = [list(i) for i in combs]\n",
    "        return combs\n",
    "        \n",
    "dataset = [list(i) for i in encoded_items]\n",
    "recommendations = Apriori(dataset).score()\n",
    "for i, v in recommendations.items():\n",
    "    if len(v) > 0:\n",
    "        for j in v:\n",
    "            items_names = ', '.join(le.inverse_transform(j[0]))\n",
    "            items_classes = ', '.join(le.inverse_transform(j[1]))\n",
    "            print('If you buy {}, you might also like {}'.format(items_names, items_classes))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
